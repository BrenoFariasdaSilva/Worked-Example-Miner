import atexit # For playing a sound when the program finishes
import concurrent.futures # For running tasks in parallel
import csv # CSV (Comma Separated Values) is a simple file format used to store tabular data, such as a spreadsheet or database
import json # For creating JSON output
import os # OS module in Python provides functions for interacting with the operating system
import pandas as pd # Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool
import shutil # Import the shutil module to perform file operations
import subprocess # The subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes
import time # This module provides various time-related functions
from colorama import Style # For coloring the terminal
from datetime import datetime # For date manipulation
from dateutil import parser # The dateutil module provides powerful extensions to the standard datetime module
from pydriller import Repository # PyDriller is a Python framework that helps developers in analyzing Git repositories. 
from tqdm import tqdm # For Generating the Progress Bars

# Imports from the repositories_picker.py file
from repositories_picker import BackgroundColors # Import the BackgroundColors class
from repositories_picker import FULL_REPOSITORIES_DIRECTORY_PATH, FULL_REPOSITORIES_LIST_JSON_FILEPATH, RELATIVE_REPOSITORIES_DIRECTORY_PATH, REPOSITORIES_SORTING_ATTRIBUTES, SOUND_FILE_PATH, START_PATH # Importing Constants from the repositories_picker.py file
from repositories_picker import create_directory, get_adjusted_number_of_threads, get_threads, output_time, path_contains_whitespaces, play_sound, setup_repository, update_sound_file_path, verbose_output, verify_filepath_exists, verify_git # Importing Functions from the repositories_picker.py file

# Default values that can be changed:
VERBOSE = False # Verbose mode. If set to True, it will output messages at the start/call of each function (Note: It will output a lot of messages).
PROCESS_JSON_REPOSITORIES = True # Process the JSON repositories. If set to True, it will process the JSON repositories, otherwise it will pick the ones defined in the DEFAULT_REPOSITORIES dictionary.

DEFAULT_REPOSITORIES = { # The default repositories to be analyzed in the format: "repository_name": "repository_url"
   "CorfuDB": "https://github.com/CorfuDB/CorfuDB",
   "kafka": "https://github.com/apache/kafka",
   "moleculer-java": "https://github.com/moleculer-java/moleculer-java",
   "scalecube-services": "https://github.com/scalecube/scalecube-services",
   "zookeeper": "https://github.com/apache/zookeeper",
}

RUN_FUNCTIONS = { # Dictionary with the functions to run and their respective booleans
   "CK Metrics": True, # Generate the CK metrics for the commits
   "Commits Information": True, # Write the commit information to a CSV file
   "Diffs": True, # Generate the diffs for the commits
   "Repositories Attributes": True, # Write the repositories attributes to a CSV file
   "Verify CK Metrics Directory": True, # Verify if the CK metrics directory is up to date
}

# File Extensions Constants:
CSV_FILE_EXTENSION = ".csv" # The extension of the file that contains the commit hashes
DIFF_FILE_EXTENSION = ".diff" # The diff file extension

# CK Constants:
CK_BRANCH = "FEAT-ClassMetric" # The branch of the CK repository to be used
CK_METRICS_FILES = ["class.csv", "method.csv"] # The files that are generated by CK

# Relative paths:
RELATIVE_CK_SUBMODULE_PATH = "../CK" # The relative path of the CK submodule
RELATIVE_CK_JAR_PATH = f"{RELATIVE_CK_SUBMODULE_PATH}/target/ck-0.7.1-SNAPSHOT-jar-with-dependencies.jar" # The relative path of the CK JAR file
RELATIVE_CK_METRICS_DIRECTORY_PATH = "/ck_metrics" # The relative path of the directory that contains the CK generated files
RELATIVE_DIFFS_DIRECTORY_PATH = "/diffs" # The relative path of the directory that contains the diffs
RELATIVE_PROGRESS_DIRECTORY_PATH = "/progress" # The relative path of the progress file
RELATIVE_REFACTORINGS_DIRECTORY_PATH = "/refactorings" # The relative path of the directory that contains the refactorings
RELATIVE_REPOSITORIES_ATTRIBUTES_FILE_PATH = f"{RELATIVE_REPOSITORIES_DIRECTORY_PATH}/repositories_attributes{CSV_FILE_EXTENSION}" # The relative path of the file that contains the repositories attributes
RELATIVE_REPOSITORY_PROGRESS_FILE_PATH = f"{RELATIVE_PROGRESS_DIRECTORY_PATH}/REPOSITORY_NAME-progress{CSV_FILE_EXTENSION}" # The relative path of the file that contains the repository progress

# Full paths (Start Path + Relative Paths):
FULL_CK_JAR_PATH = START_PATH.replace("PyDriller", "") + RELATIVE_CK_JAR_PATH.replace("../", "") # The full path of the CK JAR file
FULL_CK_METRICS_DIRECTORY_PATH = START_PATH + RELATIVE_CK_METRICS_DIRECTORY_PATH # The full path of the directory that contains the CK generated files
FULL_DIFFS_DIRECTORY_PATH = START_PATH + RELATIVE_DIFFS_DIRECTORY_PATH # The full path of the directory that contains the diffs
FULL_PROGRESS_DIRECTORY_PATH = START_PATH + RELATIVE_PROGRESS_DIRECTORY_PATH # The full path of the progress file
FULL_REFACTORINGS_DIRECTORY_PATH = START_PATH + RELATIVE_REFACTORINGS_DIRECTORY_PATH # The full path of the directory that contains the refactorings
FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH = START_PATH + RELATIVE_REPOSITORIES_ATTRIBUTES_FILE_PATH # The full path of the file that contains the repositories attributes
FULL_REPOSITORY_PROGRESS_FILE_PATH = START_PATH + RELATIVE_REPOSITORY_PROGRESS_FILE_PATH # The full path of the file that contains the repository progress
OUTPUT_DIRECTORIES = [FULL_CK_METRICS_DIRECTORY_PATH, FULL_DIFFS_DIRECTORY_PATH, FULL_REPOSITORIES_DIRECTORY_PATH] # The list of output directories

def init_and_update_submodules():
   """
   Initialize and update Git submodules

   :return: True if the Git submodules were initialized and updated successfully, False otherwise.
   """

   try:
      verbose_output(true_string=f"{BackgroundColors.GREEN}Initializing and updating the CK Git Submodule...{Style.RESET_ALL}")

      # Adjust path as necessary for reliability across environments
      submodule_path = os.path.abspath(f"{RELATIVE_CK_SUBMODULE_PATH}/.git") # Path to the ck submodule

      if not verify_filepath_exists(submodule_path): # If the submodule path does not exist
         subprocess.run(["git", "submodule", "init"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # Initialize the Git submodule
         subprocess.run(["git", "submodule", "update"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # Update the Git submodule
      else:
         subprocess.run(["git", "submodule", "update", "--remote"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # Update the Git submodule
   except subprocess.CalledProcessError as e:
      print(f"{BackgroundColors.RED}An error occurred while initializing and updating the CK Git Submodule: {e}{Style.RESET_ALL}")
      return False # Return False if the Git submodules could not be initialized and updated
   return True # Return True if the Git submodules were initialized and updated successfully

def get_current_branch(repo_path):
   """
   Retrieve the current branch of the repository at the specified path.
   
   :param repo_path: Path to the repository
   :return: The name of the current branch
   """

   result = subprocess.run(["git", "rev-parse", "--abbrev-ref", "HEAD"], cwd=repo_path, capture_output=True, text=True) # Run the Git command to get the current branch
   return result.stdout.strip() # Get the current branch

def switch_branch(target_branch, repo_path):
   """
   Switch branches in the specified repository.
   
   :param target_branch: The name of the target branch
   :param repo_path: Path to the repository
   :return: True if the branch was successfully switched, False otherwise
   """
   
   try:
      subprocess.run(["git", "checkout", target_branch], cwd=repo_path, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # Switch to the target branch
      verbose_output(true_string=f"{BackgroundColors.GREEN}Successfully switched to {BackgroundColors.CYAN}{target_branch}{BackgroundColors.GREEN} branch.{Style.RESET_ALL}")
      return True # Return True if the branch was successfully switched
   except subprocess.CalledProcessError:
      print(f"{BackgroundColors.RED}Failed to switch to {BackgroundColors.GREEN}{target_branch}{BackgroundColors.RED} branch.{Style.RESET_ALL}")
      return False # Return False if the branch could not be switched

def build_ck_jar_file(repo_path):
   """
   Build the CK JAR file if it doesn't already exist.
   
   :param repo_path: Path to the ck submodule
   :return: True if the JAR file was successfully built, False otherwise
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Building the CK JAR file...{Style.RESET_ALL}")
   subprocess.run(["mvn", "clean", "package", "-DskipTests"], cwd=repo_path, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # Build the CK JAR file
   return verify_filepath_exists(RELATIVE_CK_JAR_PATH) # Return True if the JAR file exists, False otherwise

def ensure_ck_jar_file_exists():
   """
   Ensure that the CK JAR file exists in the ck directory. If not, build the CK JAR file.
   We don't verify if the CK_JAR already exists in the target directory because we want to ensure that the latest version is used.
   
   :return: True if the CK JAR file was found or built successfully, False otherwise.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Ensuring that the {BackgroundColors.CYAN}CK JAR{BackgroundColors.GREEN} file exists in the target directory...{Style.RESET_ALL}")

   # Initialize and update Git submodules
   if not init_and_update_submodules():
      return False # Return if the Git submodules could not be initialized and updated

   ck_repo_path = os.path.abspath(RELATIVE_CK_SUBMODULE_PATH) # Path to the ck submodule

   original_branch = get_current_branch(ck_repo_path) # Store the current branch before switching

   if not switch_branch(CK_BRANCH, ck_repo_path): # Try to switch to the desired CK branch if it exists
      # If switching to CK_BRANCH fails, try to switch to a standard branch
      standard_branches = ["master", "main"] # Standard branch names
      for branch in standard_branches: # Loop through the standard branches
         if switch_branch(branch, ck_repo_path): # Try to switch to the standard branch
            original_branch = branch # Successfully switched to a standard branch, use that as the fallback
            break # Exit the loop if a standard branch was found
      else:
         print(f"{BackgroundColors.RED}None of the standard branches {BackgroundColors.CYAN}{', '.join(standard_branches)}{BackgroundColors.RED} exist in {BackgroundColors.CYAN}'ck'{BackgroundColors.RED}. Please verify the repository structure.{Style.RESET_ALL}")
         return False # Return False if no valid branch was found

   if build_ck_jar_file(ck_repo_path): # Build the JAR file if it does not exist
      switch_branch(original_branch, ck_repo_path) # Switch back to the original branch
      if verify_filepath_exists(RELATIVE_CK_JAR_PATH): # Verify if the jar exists in the ck directory
         return True # Return True if the CK JAR file was found in the target directory

   print(f"{BackgroundColors.RED}The {BackgroundColors.CYAN}CK JAR{BackgroundColors.RED} file was not found in the target directory.{Style.RESET_ALL}")
   return False # Return False if the JAR file was not found in the target directory

def verify_json_file(file_path):
   """
   Verify if the JSON file exists and is not empty.

   :param file_path: The path to the JSON file.
   :return: True if the JSON file exists and is not empty, False otherwise.
   """

   if not verify_filepath_exists(file_path): # Verify if the JSON file exists
      print(f"{BackgroundColors.RED}The repositories JSON file does not exist.{Style.RESET_ALL}")
      return False # Return False if the JSON file does not exist
   if os.path.getsize(file_path) == 0: # Verify if the JSON file is empty
      print(f"{BackgroundColors.RED}The repositories JSON file is empty.{Style.RESET_ALL}")
      return False # Return False if the JSON file is empty
   return True # Return True if the JSON file exists and is not empty

def load_repositories_from_json(file_path):
   """
   Load repositories from a JSON file.

   :param file_path: The path to the JSON file.
   :return: A dictionary containing the repositories if the JSON file is valid, None otherwise.   
   """

   try: # Try to load the JSON file
      with open(file_path, "r", encoding="utf-8") as json_file: # Open the JSON file
         repositories_list = json.load(json_file) # Load the JSON file

      if isinstance(repositories_list, list) and repositories_list: # Verify if the JSON file is a list and is not empty
         return {repo["name"]: repo["url"] for repo in repositories_list} # Return the dictionary containing the repositories
      else:
         print(f"{BackgroundColors.RED}The repositories JSON file is not in the correct format.{Style.RESET_ALL}")
         return None # Return None if the JSON file is not in the correct format
   except (json.JSONDecodeError, KeyError) as e: # Handle the exception if there is an error parsing the JSON file
      verbose_output(true_string=f"{BackgroundColors.RED}Error parsing the repositories JSON file: {e}{Style.RESET_ALL}", is_error=True)
      return None # Return None if there is an error parsing the JSON file

def update_repositories_dictionary():
   """
   Update the repositories list in the DEFAULT_REPOSITORIES dictionary.
   
   :return: True if the DEFAULT_REPOSITORIES dictionary was successfully updated with values from the JSON file, False otherwise.
   """
   
   verbose_output(true_string=f"{BackgroundColors.GREEN}Updating the repositories list file with the DEFAULT_REPOSITORIES dictionary...{Style.RESET_ALL}")

   global DEFAULT_REPOSITORIES # Use the global DEFAULT_REPOSITORIES variable

   for sorting_attribute in REPOSITORIES_SORTING_ATTRIBUTES: # Iterate through all REPOSITORIES_SORTING_ATTRIBUTES to find a valid file
      filename = FULL_REPOSITORIES_LIST_JSON_FILEPATH.replace("SORTING_ATTRIBUTE", sorting_attribute) # Get the filename for the current attribute
      
      if not verify_filepath_exists(filename): # If file doesn't exist, skip to the next attribute
         verbose_output(true_string=f"{BackgroundColors.YELLOW}File {filename} not found, checking next...{Style.RESET_ALL}")
         continue # Skip to the next attribute
      
      if not verify_json_file(filename): # If the JSON file is not valid, skip to the next
         verbose_output(true_string=f"{BackgroundColors.RED}Invalid JSON file: {filename}, checking next...{Style.RESET_ALL}")
         continue # Skip to the next attribute
      
      json_repositories = load_repositories_from_json(filename) # Verify if the JSON file exists
      if not json_repositories: # If loading the JSON file fails, skip to the next
         verbose_output(true_string=f"{BackgroundColors.RED}Failed to load JSON data from {filename}, checking next...{Style.RESET_ALL}")
         continue # Skip to the next attribute

      DEFAULT_REPOSITORIES = json_repositories # Update DEFAULT_REPOSITORIES and return True if successful
      verbose_output(true_string=f"{BackgroundColors.GREEN}The {BackgroundColors.CLEAR_TERMINAL}DEFAULT_REPOSITORIES{BackgroundColors.GREEN} dictionary was successfully updated from {BackgroundColors.CYAN}{filename}{BackgroundColors.GREEN}.{Style.RESET_ALL}")
      return True # Return True if the DEFAULT_REPOSITORIES dictionary was successfully updated with values from the JSON file
   
   # If no valid files were found
   print(f"{BackgroundColors.RED}No valid JSON files found for any of the sorting attributes.{Style.RESET_ALL}")
   return False # Return False if no valid JSON files were found

def verify_repositories_execution_constants():
   """
   Verify the constants used in the execution of the repositories.
   It will process the JSON repositories, if the PROCESS_JSON_REPOSITORIES constant is set to True or if the DEFAULT_REPOSITORIES dictionary is empty.
   
   :return: None
   """

   if PROCESS_JSON_REPOSITORIES or not DEFAULT_REPOSITORIES: # Verify if PROCESS_REPOSITORIES_LIST is set to True or if the DEFAULT_REPOSITORIES dictionary is empty
      if not update_repositories_dictionary(): # Update the repositories list
         print(f"{BackgroundColors.RED}The repositories list could not be updated. Please execute the {BackgroundColors.CYAN}repositories_picker.py{BackgroundColors.RED} script with the {BackgroundColors.CYAN}PROCESS_JSON_REPOSITORIES{BackgroundColors.RED} set to {BackgroundColors.CYAN}False{BackgroundColors.RED} or manually fill the {BackgroundColors.CYAN}DEFAULT_REPOSITORIES{BackgroundColors.RED} dictionary.{Style.RESET_ALL}")
         exit() # Exit the program if the repositories list could not be updated

def get_commit_filepaths(commit_file_path):
   """
   Read the commit information from a CSV file and return a list of file paths in the format '{Commit Number}-{Commit Hash}'.

   :param commit_file_path: Path to the CSV file containing commit details
   :return: List of file paths in the format '{Commit Number}-{Commit Hash}'
   """
   
   if not verify_filepath_exists(commit_file_path): # Verify if the file exists
      return [] # Return an empty list if the file does not exist

   df = pd.read_csv(commit_file_path, sep=",", usecols=["Commit Number", "Commit Hash"], header=0) # Read the CSV file and get the necessary columns
   filepaths = [f"{row['Commit Number']}-{row['Commit Hash']}" for _, row in df.iterrows()] # Create a list of file paths in the format '{Commit Number}-{Commit Hash}'
   
   return filepaths # Return the list of file paths

def verify_ck_metrics_files(folder_path, ck_metrics_files=CK_METRICS_FILES):
   """
   Verify if all the CK metrics files exist inside the specified folder.

   :param folder_path: Path to the folder containing CK metrics files
   :param ck_metrics_files: List of CK metrics file names to check
   :return: True if all CK metrics files exist, False otherwise
   """

   for ck_metric_file in ck_metrics_files: # Loop through the CK metrics files
      ck_metric_file_path = os.path.join(folder_path, ck_metric_file) # Join the folder path with the CK metric file
      if not verify_filepath_exists(ck_metric_file_path): # If the CK metric file does not exist
         return False # Return False if the CK metric file does not exist
   return True # Return True if all CK metrics files exist

def is_local_repository_metrics_outdated(number_of_commits, total_commits_processed, repository_name):
   """
   Verify if the repository is outdated based on the number of unprocessed commits
   and the total number of commits. If more than 100 commits are unprocessed, mark
   the repository as outdated and update global flags.

   :param number_of_commits: The total number of commits in the repository.
   :param total_commits_processed: The number of commits that have already been processed.
   :param repository_name: The name of the repository being checked.
   :return: The number of unprocessed commits.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Checking if the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository is outdated...{Style.RESET_ALL}")

   global RUN_FUNCTIONS # Access the global RUN_FUNCTIONS

   unprocessed_commits = number_of_commits - total_commits_processed # Calculate the number of unprocessed commits
   if unprocessed_commits > 0: # If more than 100 commits are unprocessed
      RUN_FUNCTIONS["CK Metrics"] = True # Set CK metrics generation to True globally
   return unprocessed_commits # Return the number of unprocessed commits

def verify_commit_files_exist(repo_path, commit_filepaths):
   """
   Verifies that each commit in the list of commit filepaths has its corresponding folder and CK metrics files.

   :param repo_path: The base path of the repository's CK metrics folder.
   :param commit_filepaths: List of commit filepaths.
   :return: True if all files exist, False otherwise.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Verifying if all commit files exist in the {BackgroundColors.CYAN}{repo_path}{BackgroundColors.GREEN} directory...{Style.RESET_ALL}")

   missing_files_count = 0 # Initialize the count of non-existing folders or files

   for ck_metrics_filepath in commit_filepaths: # Loop through the commit filepaths
      folder_path = os.path.join(repo_path, ck_metrics_filepath) # Full path to the commit's metrics folder

      if verify_filepath_exists(folder_path): # Verify if the folder exists
         if not verify_ck_metrics_files(folder_path, CK_METRICS_FILES): # Verify if all CK metrics files exist
            missing_files_count += 1 # Increment the count of invalid folders
      else: # If the folder does not exist
         missing_files_count += 1 # Increment the count of non-existing folders

   return missing_files_count # Return the count of non-existing folders or files

def verify_ck_metrics_directory(repository_name, repository_url, number_of_commits):
   """
   Verifies if all the metrics are already calculated and if the CK metrics repository directory is up to date by comparing the number of commits and processed commits. If metrics are up to date or there are unprocessed commits, the function returns True to allow further processing.

   :param repository_name: Name of the repository to be analyzed.
   :param repository_url: URL of the repository to be analyzed.
   :param number_of_commits: Number of commits to be analyzed. If 0, the total will be calculated from the repository.
   :return: Tuple (bool, int) indicating if metrics are up to date or need further processing, and the number of unprocessed commits.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Verifying if the metrics for {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} are calculated and up to date...{Style.RESET_ALL}")

   number_of_commits = sum(1 for _ in Repository(repository_url).traverse_commits()) if number_of_commits == 0 else number_of_commits # Get the total number of commits if not provided

   repo_path = os.path.join(FULL_CK_METRICS_DIRECTORY_PATH, repository_name) # Full path to the repository's metrics directory
   commit_file = f"{repository_name}-commits_list{CSV_FILE_EXTENSION}" # The commit hashes file name
   commit_file_path = os.path.join(FULL_CK_METRICS_DIRECTORY_PATH, commit_file) # Full path to the commit hashes file

   repository_ck_metrics_filepaths = get_commit_filepaths(commit_file_path) # Get the list of commit filepaths from the commit hashes file

   if not repository_ck_metrics_filepaths: # If the list of commit filepaths is empty
      print(f"{BackgroundColors.RED}The list of commits for {BackgroundColors.CYAN}{repository_name}{BackgroundColors.RED} is empty in the {BackgroundColors.CYAN}{commit_file}{BackgroundColors.RED} file.{Style.RESET_ALL}")
      return False, number_of_commits # Return False if the list is empty

   missing_files_count = verify_commit_files_exist(repo_path, repository_ck_metrics_filepaths) # Verify if all commit files exist
   if missing_files_count > 0: # If there are missing commit files
      print(f"{BackgroundColors.RED}The {BackgroundColors.CYAN}{repository_name}{BackgroundColors.RED} repository is missing {BackgroundColors.CYAN}{missing_files_count}{BackgroundColors.RED} commit files.{Style.RESET_ALL}")
      return False, missing_files_count # Return False if any commit metrics folder/files are missing
   
   total_commits_processed = len(repository_ck_metrics_filepaths) # Get the total number of commits processed
   unprocessed_commits = is_local_repository_metrics_outdated(number_of_commits, total_commits_processed, repository_name) # Verify if the repository is outdated

   if unprocessed_commits > 0: # Verify if there are unprocessed commits, even if it's below the threshold
      return True, unprocessed_commits # Allow processing to continue for unprocessed commits
   
   print(f"{BackgroundColors.GREEN}The {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository metrics are up to date.{Style.RESET_ALL}")
   return True, 0 # All metrics are calculated, no unprocessed commits

def read_progress_file(file_path):
   """
   Read the contents of the progress file using CSV reader.

   :param file_path: Path to the saved progress file
   :return: List of dictionaries representing the commit information from the progress file
   """

   if not verify_filepath_exists(file_path): # Verify if the file exists
      return [] # Return an empty list if the file does not exist

   try: # Try to read the progress file
      with open(file_path, "r") as file: # Open the progress file to read
         reader = csv.DictReader(file) # Create a CSV reader
         lines = list(reader) # Convert the reader object into a list of dictionaries
   except Exception as e: # Handle exceptions
      print(f"{BackgroundColors.RED}An error occurred while reading the progress file: {e}{Style.RESET_ALL}")
      return [] # Return an empty list if an error occurs

   return lines # Return the list of dictionaries representing the commit information

def parse_commit_info(lines):
   """
   Parse commit information from the lines of the progress file.

   :param lines: List of dictionaries from the progress file
   :return: Tuple containing the list of commit info, the last commit number, and the last commit hash
   """

   commits_info = [] # List to store the commit information
   last_commit_number = 0 # Variable to store the last commit number
   last_commit_hash = None # Variable to store the last commit hash

   if lines: # Ensure there are lines to process
      for line in lines: # Process each line in the CSV
         try: # Try to parse the line
            commit_number = int(line["Commit Number"]) # Read the commit number to an integer
            commit_hash = line["Commit Hash"] # Get the commit hash
            commit_message = f'"{line["Commit Message"].split("\n")[0].strip()}"' # Format the commit message to only keep the first line, stripped of leading/trailing spaces
            commit_date = parser.parse(line["Commit Date"]) # Use dateutil.parser to handle the date with timezone
            lines_added = int(line["Lines Added"]) # Read the lines added to an integer
            lines_removed = int(line["Lines Removed"]) # Read the lines removed to an integer
            commit_code_churn = int(line["Commit Code Churn"]) # Read the commit code churn to an integer
            code_churn_avg_per_file = round(float(line["Code Churn Avg Per File"]), 2) # Round the code churn average per file to 2 decimal places
            modified_files_count = int(line["Modified Files Count"]) # Read the modified files count to an integer
            commit_url = line["Commit URL"] # Get the commit URL

            commit_tuple = (commit_number, commit_hash, commit_message, commit_date, lines_added, lines_removed, commit_code_churn, code_churn_avg_per_file, modified_files_count, commit_url) # Create a tuple with the commit information
            commits_info.append(commit_tuple) # Append the commit tuple to the list

         except KeyError as e: # Handle missing keys
            print(f"Missing key in line: {e}, line data: {line}")
         except Exception as e: # Handle parsing errors
            print(f"Error parsing line: {e}, line data: {line}")

      if commits_info: # If there are commits in the list
         last_commit_number = commits_info[-1][0] # Commit number from the last line
         last_commit_hash = commits_info[-1][1] # Commit hash from the last line
   
   return commits_info, last_commit_number, last_commit_hash # Return the list of commit information, the last commit number, and the last commit hash

def calculate_percentage_progress(last_commit_number, total_commits):
   """
   Calculate the percentage progress based on the last commit number.

   :param last_commit_number: The last processed commit number
   :param total_commits: Total number of commits to be processed
   :return: Percentage of progress as a rounded float
   """

   return round((last_commit_number / total_commits) * 100, 2) # Calculate the percentage progress

def write_progress_file(file_path, commits_tuple_list):
   """
   Write the provided lines to the progress file, including the header.

   :param file_path: Path to the saved progress file
   :param commits_tuple_list: List of commit information tuples
   """

   with open(file_path, "w") as file: # Open the progress file to write
      writer = csv.writer(file, quotechar=None) # Create a CSV writer
      writer.writerow(["Commit Number", "Commit Hash", "Commit Message", "Commit Date", "Lines Added", "Lines Removed", "Commit Code Churn", "Code Churn Avg Per File", "Modified Files Count", "Commit URL"]) # Write the header
      for commit_tuple in commits_tuple_list: # Loop through the commits tuple list
         writer.writerow(commit_tuple) # Write the current commit tuple to the CSV file

def get_last_execution_progress(repository_name, saved_progress_file, number_of_commits):
   """
   Gets the last execution progress of the repository.

   :param repository_name: Name of the repository to be analyzed.
   :param saved_progress_file: Name of the file that contains the saved progress.
   :param number_of_commits: Number of commits to be analyzed.
   :return: The commits_info, last_commit_number, and last_commit_hash.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Getting the last execution progress of the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository...{Style.RESET_ALL}")

   lines = read_progress_file(saved_progress_file) # Read the progress file
   commits_info = [], last_execution_progress = [0, None] # Initialize the variables

   if lines: # If there are lines in the progress file
      commits_info, last_execution_progress[0], last_execution_progress[1] = parse_commit_info(lines) # Parse the commit information
      percentage_progress = calculate_percentage_progress(last_execution_progress[0], number_of_commits) # Calculate the percentage progress
      print(f"{BackgroundColors.GREEN}{BackgroundColors.CYAN}{repository_name.capitalize()}{BackgroundColors.GREEN} stopped executing at {BackgroundColors.CYAN}{percentage_progress}%{BackgroundColors.GREEN} of its progress in the {BackgroundColors.CYAN}{last_execution_progress[0]}ยบ{BackgroundColors.GREEN} commit: {BackgroundColors.CYAN}{last_execution_progress[0]}{BackgroundColors.GREEN}.{Style.RESET_ALL}")
      execution_time = f"{BackgroundColors.GREEN}Estimated time for running the remaining iterations in {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN}: {Style.RESET_ALL}"
      output_time(execution_time, number_of_commits - last_execution_progress[0]) # Output estimated time for remaining iterations
   else:
      write_progress_file(saved_progress_file, commits_info) # Create the file if no progress exists

   return commits_info, last_execution_progress # Return the commits_info and last_commit_number

def get_repository_attributes(repository_name, number_of_commits, elapsed_time):
   """
   Retrieves repository attributes such as the number of classes, lines of code, and directory sizes.

   :param repository_name: Name of the repository.
   :param number_of_commits: Number of commits to be analyzed.
   :param elapsed_time: Elapsed time of the execution.
   :return: A dictionary with repository attributes.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Retrieving the repository attributes...{Style.RESET_ALL}")

   output_directory = os.path.join(FULL_CK_METRICS_DIRECTORY_PATH, repository_name) # The path to the CK metrics directory
   sorted_dirs = get_filtered_sorted_directories(output_directory) # Get and sort directories

   last_directory = get_last_directory(sorted_dirs) # Get the last directory
   last_directory_path = os.path.join(output_directory, last_directory) # Update the output directory with the last directory

   total_classes, total_lines_of_code = get_classes_count_and_loc_metrics(last_directory_path) # Get the total number of classes and lines of code

   # Get the size of the output directories in GB and the progress file size in GB
   output_dirs_size = get_directories_size_in_gb(repository_name, OUTPUT_DIRECTORIES) + get_file_size_in_gb(FULL_REPOSITORY_PROGRESS_FILE_PATH.replace("REPOSITORY_NAME", repository_name))

   return { # Return the repository attributes dictionary
      "repository_name": repository_name, # Name of the repository
      "classes": total_classes, # Total number of classes
      "lines_of_code": total_lines_of_code, # Total number of lines of code
      "commits": number_of_commits, # Total number of commits
      "execution_time_in_minutes": round(elapsed_time / 60, 2), # Execution time in minutes
      "size_in_gb": round(output_dirs_size, 2) # Size of the output directories in GB
   }

def calculate_code_churn(commit):
   """"
   Calculate the code churn for a commit.
   
   :param commit: The commit object to be analyzed.
   :return: Tuple containing the lines added, lines removed, and total code churn.
   """

   lines_added = sum(file.added_lines for file in commit.modified_files) # Calculate the total number of lines added
   lines_removed = sum(file.deleted_lines for file in commit.modified_files) # Calculate the total number of lines removed
   return lines_added, lines_removed, lines_added + lines_removed # Return the lines added, lines removed, and total code churn

def generate_diffs(repository_name, commit, commit_number):
   """
   Generates the diffs for the commits of a repository.

   :param repository_name: Name of the repository to be analyzed.
   :param commit: The commit object to be analyzed.
   :param commit_number: Number of the commit to be analyzed.
   :return: None
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Generating the diffs for the {BackgroundColors.CYAN}{commit_number}ยบ{BackgroundColors.GREEN} commit of the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository...{Style.RESET_ALL}")

   for modified_file in commit.modified_files: # Loop through the modified files of the commit
      file_diff = modified_file.diff # Get the diff of the modified file

      diff_file_directory = f"{START_PATH}{RELATIVE_DIFFS_DIRECTORY_PATH}/{repository_name}/{commit_number}-{commit.hash}/" # Define the directory to save the diff file

      if not verify_filepath_exists(diff_file_directory): # Verify if the directory does not exist
         os.makedirs(diff_file_directory, exist_ok=True) # Create the directory

      if not verify_filepath_exists(f"{diff_file_directory}{modified_file.filename}{DIFF_FILE_EXTENSION}"): # Verify if the diff file does not exist
         with open(f"{diff_file_directory}{modified_file.filename}{DIFF_FILE_EXTENSION}", "w", encoding="utf-8", errors="ignore") as diff_file: # Open the diff file to write the diff
            diff_file.write(file_diff) # Write the diff to the file

def checkout_branch(branch_name):
   """
   Checks out a specific branch.

   :param branch_name: Name of the branch to be checked out.
   :return: None
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Checking out the {BackgroundColors.CYAN}{branch_name}{BackgroundColors.GREEN} branch...{Style.RESET_ALL}")

   checkout_thread = subprocess.Popen(["git", "checkout", branch_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # Run the Git command to checkout the branch
   checkout_thread.wait() # Wait for the thread to finish

def generate_output_directory_paths(repository_name, commit_number, commit_hash):
   """
   Generates the output directory path for the CK metrics generator.

   :param repository_name: Name of the repository to be analyzed.
   :param commit_number: Number of the commit to be analyzed.
   :param commit_hash: Commit hash of the commit to be analyzed.
   :return: The output_directory and relative_output_directory paths.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Generating the output directory paths...{Style.RESET_ALL}")

   output_directory = f"{FULL_CK_METRICS_DIRECTORY_PATH}/{repository_name}/{commit_number}-{commit_hash}/" # Define the output directory path
   relative_output_directory = f"{RELATIVE_CK_METRICS_DIRECTORY_PATH}/{repository_name}/{commit_number}-{commit_hash}/" # Define the relative output directory path

   return output_directory, relative_output_directory # Return the output_directory and relative_output_directory paths
   
def run_ck_metrics_generator(cmd):
   """
   Runs the CK metrics generator in a subprocess.

   :param cmd: Command to be executed.
   :return: None
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Running the CK Metrics Generator Command...{Style.RESET_ALL}")

   thread = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE) # Run the CK metrics generator command
   stdout, stderr = thread.communicate() # Get the stdout and stderr of the thread

def get_classes_count_and_loc_metrics(output_directory):
   """
   Extracts the number of classes and lines of code from the CK output files.

   :param output_directory: Path to the directory containing the CK metrics files.
   :return: Tuple (classes, lines_of_code).
   """

   class_count = 0 # Total number of classes
   loc_count = 0 # Total number of lines of code

   file_path = f"{output_directory}/class.csv" # Path to the CK metrics CSV file
   if not verify_filepath_exists(file_path): # Verify if the directory exists
      print(f"{BackgroundColors.RED}The {BackgroundColors.CYAN}class.csv{BackgroundColors.RED} file does not exist in the {BackgroundColors.CYAN}{output_directory}{BackgroundColors.RED} directory.{Style.RESET_ALL}")
      return class_count, loc_count # Return the class count and lines of code count
   
   with open(file_path, "r") as file: # Open the CK metrics CSV file to extract the metrics
      reader = csv.DictReader(file) # Read the CSV file
      for row in reader: # Loop through the rows of the CSV file
         class_count += 1 # Increment the class count
         loc_count += int(row["loc"]) # Increment the lines of code count

   return class_count, loc_count # Return the class count and lines of code count

def sum_directory_files_size(directory):
   """
   Calculates the total size of the given directory in bytes.

   :param directory: Path to the directory.
   :return: Total size of the directory in bytes.
   """

   total_size = 0 # Total size of the directory
   for dirpath, dirnames, filenames in os.walk(directory): # Walk through the directory
      for filename in filenames: # Loop through the filenames
         filepath = os.path.join(dirpath, filename) # Join the directory path with the filename
         total_size += os.path.getsize(filepath) # Get the size of the file and add it to the total size
   return total_size # Return the total size of the directory

def show_execution_time(first_iteration_duration, elapsed_time, number_of_commits, repository_name):
   """
   Shows the execution time of the CK metrics generator.

   :param first_iteration_duration: Duration of the first iteration.
   :param elapsed_time: Elapsed time of the execution.
   :param number_of_commits: Number of commits to be analyzed.
   :param repository_name: Name of the repository to be analyzed.
   :return: None
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Showing the execution time of the CK metrics generator...{Style.RESET_ALL}")

   estimated_time_string = f"{BackgroundColors.GREEN}Estimated time for running all the of the iterations in {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN}: "
   output_time(estimated_time_string, round(first_iteration_duration * number_of_commits, 2)) # Output the estimated time for running all of the iterations for the repository
   time_taken_string = f"{BackgroundColors.GREEN}Time taken to generate CK metrics for {BackgroundColors.CYAN}{number_of_commits}{BackgroundColors.GREEN} commits in {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository: "
   output_time(time_taken_string, round(elapsed_time, 2)) # Output the time taken to generate CK metrics for the commits in the repository

def get_filtered_sorted_directories(directory_path):
   """
   Get and sort directories by the commit number.

   :param directory_path: Path to the directory containing subdirectories
   :return: List of sorted directories by commit number
   """

   dirnames = os.listdir(directory_path) # Get the directory names
   filtered_dirs = [dirname for dirname in dirnames if "-" in dirname and dirname.split("-")[0].isdigit()] # Filter directories
   return sorted(filtered_dirs, key=lambda dirname: int(dirname.split("-")[0])) # Sort by commit number

def get_last_directory(dirs):
   """
   Get the last directory from a sorted list of directories.

   :param dirs: List of sorted directories
   :return: Last directory in the list
   """

   return dirs[-1] if dirs else "" # Return the last directory if the list is not empty

def get_directory_size_in_gb(directory_path):
   """
   Get the size of a directory in GB.

   :param directory_path: Path to the directory
   :return: Size of the directory in GB
   """

   return sum_directory_files_size(directory_path) / (1024 ** 3) # Size in GB

def get_directories_size_in_gb(repository_name, directories=OUTPUT_DIRECTORIES):
   """
   Get the size of the directories in GB.

   :param repository_name: Name of the repository.
   :return: Total size of the output directories in GB
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Getting the size of the output directories in {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository...{Style.RESET_ALL}")

   output_dirs_size = 0 # Total size of the output directories in GB
   for output_dir in directories: # Loop through the output directories
      repository_output_dir = os.path.join(output_dir, repository_name) # Update the output directory with the repository name
      if verify_filepath_exists(os.path.join(output_dir, repository_name)): # Verify if the directory exists
         output_dirs_size += get_directory_size_in_gb(repository_output_dir) # Get the size of each output directory in GB
   
   return output_dirs_size # Return the total size of the output directories in GB

def get_file_size_in_gb(filepath):
   """
   Returns the size of the file in GB.
   If the file does not exist, the size is 0 GB.

   :param filepath: Path to the file
   :return: float, size of the progress file in GB
   """

   try: # Try to get the size of the progress file
      file_size = os.path.getsize(filepath) / (1024 ** 3) # Size in GB
   except FileNotFoundError:
      file_size = 0 # File does not exist, size is 0 GB
   return file_size # Return the size of the progress file in GB

def traverse_repository(repository_name, repository_url, number_of_commits):
   """
   Traverses the repository to run CK for every commit hash in the repository.
   Tracks the processing time, number of classes, lines of code, and other attributes.

   :param repository_name: Name of the repository to be analyzed.
   :param repository_url: URL of the repository to be analyzed.
   :param number_of_commits: Number of commits to be analyzed.
   :return: A tuple with the commits information list and a dictionary with repository attributes.
   """

   verbose_output(true_string=f"{BackgroundColors.GREEN}Traversing the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository to run CK for every commit hash...{Style.RESET_ALL}")

   start_time = time.time() # Start measuring time
   first_iteration_duration = 0 # Duration of the first iteration
   saved_progress_file = FULL_REPOSITORY_PROGRESS_FILE_PATH.replace("REPOSITORY_NAME", repository_name) # The path to the saved progress file

   commits_info, last_execution_progress = get_last_execution_progress(repository_name, saved_progress_file, number_of_commits) # Get the last execution progress of the repository
   commit_number = 1 if last_execution_progress[0] == 0 else last_execution_progress[0] + 1 # Set the commit number to 1 if the last commit number is 0, otherwise increment the last commit number

   if last_execution_progress[0] == number_of_commits: # Return if the last commit number is equal to the total number of commits
      return commits_info, get_repository_attributes(repository_name, number_of_commits, first_iteration_duration) # Return the commits info and repository attributes

   # Create a progress bar with the total number of commits
   with tqdm(total=number_of_commits - last_execution_progress[0], unit=f"{BackgroundColors.GREEN}Traversing the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} commit tree{Style.RESET_ALL}", unit_scale=True) as pbar:
      for commit in Repository(repository_url, from_commit=last_execution_progress[1]).traverse_commits(): # Loop through the commits of the repository
         lines_added, lines_removed, code_churn = calculate_code_churn(commit) # Calculate the code churn for the commit
         modified_files_count = len(commit.modified_files) # Number of modified files
         code_churn_avg_per_file = code_churn / modified_files_count if modified_files_count > 0 else 0 # Code churn average per file
         current_tuple = (commit_number, commit.hash, f'"{commit.msg.split("\n")[0].strip().replace("\"", "")}"', commit.committer_date, lines_added, lines_removed, code_churn, round(code_churn_avg_per_file, 2), modified_files_count, f'"{repository_url}/commit/{commit.hash}"') # Create a tuple with the commit information
         commits_info.append(current_tuple) # Append the current tuple to the commits_info list

         generate_diffs(repository_name, commit, commit_number) if RUN_FUNCTIONS["Diffs"] else None # Save the diff of the modified files of the current commit

         if not verify_ck_metrics_files(f"{FULL_CK_METRICS_DIRECTORY_PATH}/{repository_name}/{commit_number}-{commit.hash}", CK_METRICS_FILES): # Verify if the CK metrics files do not exist
            workdir = f"{FULL_REPOSITORIES_DIRECTORY_PATH}/{repository_name}" # The path to the repository directory
            os.chdir(workdir) # Change working directory to the repository directory

            checkout_branch(commit.hash) # Checkout the current commit hash branch to run ck

            output_directory, relative_output_directory = generate_output_directory_paths(repository_name, commit_number, commit.hash) # Generate the output directory paths
            create_directory(output_directory, relative_output_directory) # Create the ck_metrics directory

            os.chdir(output_directory) # Change working directory to the repository directory

            cmd = f"java -jar {FULL_CK_JAR_PATH} {workdir} false 0 false {output_directory} true" # The command to run the CK metrics generator
            run_ck_metrics_generator(cmd) if RUN_FUNCTIONS["CK Metrics"] else None # Run the CK metrics generator

         if commit_number == 1: # If it is the first iteration
            first_iteration_duration = time.time() - start_time # Calculate the duration of the first iteration

         with open(saved_progress_file, "a") as progress_file: # Open the progress file to append
            progress_file.write(f",".join(map(str, current_tuple)) + "\n") # Write the current tuple to the progress file
         
         commit_number += 1 # Increment the commit number
         pbar.update(1) # Update the progress bar

   elapsed_time = time.time() - start_time # Calculate elapsed time
   show_execution_time(first_iteration_duration, elapsed_time, number_of_commits - last_execution_progress[0], repository_name) # Show the execution time of the CK metrics generator

   return commits_info, get_repository_attributes(repository_name, number_of_commits, elapsed_time) # Return the commits info and repository attributes

def write_commits_information_to_csv(repository_name):
   """
   Moves the saved progress file to a new path with a new name.

   :param repository_name: Name of the repository to be analyzed.
   :return: None
   """
   
   verbose_output(true_string=f"{BackgroundColors.GREEN}Moving the commit information to a new csv file...{Style.RESET_ALL}")
   
   saved_progress_filepath = FULL_REPOSITORY_PROGRESS_FILE_PATH.replace("REPOSITORY_NAME", repository_name) # Original path to the saved progress file
   commits_list_filepath = f"{FULL_CK_METRICS_DIRECTORY_PATH}/{repository_name}-commits_list{CSV_FILE_EXTENSION}" # The path to the CSV file

   try: # Try to move the file the saved progress file (generated bin the traverse_repository function) to the new path with a new name
      shutil.move(saved_progress_filepath, commits_list_filepath) # Move the file
      verbose_output(true_string=f"{BackgroundColors.GREEN}The {BackgroundColors.CYAN}{repository_name}-commits_list{CSV_FILE_EXTENSION}{BackgroundColors.GREEN} file was successfully moved from {BackgroundColors.CYAN}{saved_progress_filepath}{BackgroundColors.GREEN} to {BackgroundColors.CYAN}{commits_list_filepath}{BackgroundColors.GREEN}.{Style.RESET_ALL}")
   except Exception as e: # Handle exceptions
      print(f"{BackgroundColors.RED}An error occurred while moving the commit information to the {BackgroundColors.CYAN}{repository_name}-commits_list{CSV_FILE_EXTENSION}{BackgroundColors.RED} file: {e}{Style.RESET_ALL}")

def update_repository_attributes(repository_attributes):
   """
   Updates or adds repository attributes in the CSV file.

   :param repository_attributes: Dictionary containing the repository attributes.
   :return: Tuple containing the updated rows and a boolean indicating if the file exists.
   """
   
   file_exists = verify_filepath_exists(FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH) # Verify if the file exists

   updated_rows = [] # List to store the updated rows
   repository_found = False # Boolean to indicate if the repository was found

   if file_exists: # Read existing data if the file exists
      with open(FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH, "r", newline="") as csv_file: # Open the CSV file to read
         reader = csv.reader(csv_file) # Create a CSV reader
         for row in reader: # Loop through the rows of the CSV file
            if row and row[0].lower() == repository_attributes["repository_name"].lower(): # If the repository was found
               updated_row = [repository_attributes["repository_name"], int(row[1]) + repository_attributes["classes"], int(row[2]) + repository_attributes["lines_of_code"], int(row[3]) + repository_attributes["commits"], round(float(row[4]) + repository_attributes["execution_time_in_minutes"], 2), round(float(row[5]) + repository_attributes["size_in_gb"], 2)] # Update existing row with new attributes
               updated_rows.append(updated_row) # Append the updated row
               repository_found = True # Mark as found
            else: # If the repository was not found
               updated_rows.append(row) # Keep existing row

   if not repository_found: # If repository was not found, prepare a new row
      updated_rows.append([repository_attributes["repository_name"], repository_attributes["classes"], repository_attributes["lines_of_code"], repository_attributes["commits"], repository_attributes["execution_time_in_minutes"], repository_attributes["size_in_gb"]]) # Append the new row

   return updated_rows, file_exists # Return the updated rows and if the file exists

def verify_csv_header_exists(filepath, header):
   """
   Verifies if the CSV file exists and if the header is present.

   :param filepath: Path to the CSV file.
   :return: True if the header exists, otherwise False.
   """
   
   header_presence = False # Boolean to indicate if the header is present

   try:
      with open(filepath, "r") as file: # Open the CSV file to read, if it exists
         first_line = file.readline().strip() # Read the first line of the file
         if first_line == ",".join(header): # If the header is present
            header_presence = True # Mark as present 
   except FileNotFoundError:
      pass # File does not exist

   return header_presence # Return if the header is present

def write_repositories_attributes_to_csv(repository_attributes):
   """
   Writes the repository attributes to a CSV file.

   :param repository_attributes: Dictionary containing the repositories attributes.
   :return: None
   """
   
   verbose_output(true_string=f"{BackgroundColors.GREEN}Writing the {repository_attributes['repository_name']} attributes to the {FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH} file...{Style.RESET_ALL}")

   updated_rows, file_exists = update_repository_attributes(repository_attributes) # Update or add repository attributes and get updated rows

   header = ["Repository Name", "Number of Classes", "Lines of Code (LOC)", "Number of Commits", "Execution Time (Minutes)", "Size (GB)"] # Header of the CSV file
   header_exists = verify_csv_header_exists(FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH, header) # Verify if the header exists

   with open(FULL_REPOSITORIES_ATTRIBUTES_FILE_PATH, "w", newline="") as csv_file: # Open the CSV file to write
      writer = csv.writer(csv_file) # Create a CSV writer
      if not file_exists or not header_exists: # If the file does not exist or the header is missing
         writer.writerow(header) # Write the header
      writer.writerows(updated_rows) # Write updated rows

def process_repository(repository_name, repository_url, number_of_commits):
   """
   Processes the repository.

   :param repository_name: Name of the repository to be analyzed.
   :param repository_url: URL of the repository to be analyzed.
   :param number_of_commits: Number of commits to be analyzed.
   :return: None
   """

   print(f"{BackgroundColors.GREEN}Processing the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository...{Style.RESET_ALL}")

   create_directory(FULL_CK_METRICS_DIRECTORY_PATH, RELATIVE_CK_METRICS_DIRECTORY_PATH) # Create the ck metrics directory
   create_directory(FULL_PROGRESS_DIRECTORY_PATH, RELATIVE_PROGRESS_DIRECTORY_PATH) # Create the progress directory
   create_directory(FULL_REPOSITORIES_DIRECTORY_PATH, RELATIVE_REPOSITORIES_DIRECTORY_PATH) # Create the repositories directory

   setup_repository(repository_name, repository_url) # Setup the repository: Clone or update the repository

   commits_info, repository_attributes = traverse_repository(repository_name, repository_url, number_of_commits) # Traverse the repository to run CK for every commit hash in the repository

   write_commits_information_to_csv(repository_name) if RUN_FUNCTIONS["Commits Information"] else None # Write the commits information to a CSV file

   write_repositories_attributes_to_csv(repository_attributes) if RUN_FUNCTIONS["Repositories Attributes"] else None # Save repository attributes to a CSV file

   checkout_branch("main") # Checkout the main branch

def setup_process_repository(repository_name, repository_url, number_of_commits=None):
   """
   Verifies if the CK metrics directory exists and if there are unprocessed commits.

   :param repository_name: Name of the repository to be analyzed.
   :param repository_url: URL of the repository to be analyzed.
   :return: True if the repository is up to date, False if further processing is required.
   """

   global RUN_FUNCTIONS # Declare the RUN_FUNCTIONS as a global variable
   number_of_commits = sum(1 for _ in Repository(repository_url).traverse_commits()) if number_of_commits == None else number_of_commits

   if RUN_FUNCTIONS["Verify CK Metrics Directory"]: # If the function to verify the CK metrics directory is enabled
      ck_metrics_files_exist, unprocessed_commits = verify_ck_metrics_directory(repository_name, repository_url, number_of_commits)

      if ck_metrics_files_exist: # If metrics directory exists and is either up to date or has unprocessed commits
         if unprocessed_commits <= 0: # No unprocessed commits, all metrics are up to date
            print(f"{BackgroundColors.GREEN}The {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository is fully up to date with {BackgroundColors.CYAN}{number_of_commits}{BackgroundColors.GREEN} commits processed.{Style.RESET_ALL}")
            return # Return if everything is already calculated
         else: # There are unprocessed commits, continue processing
            print(f"{BackgroundColors.GREEN}Processing the {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN} repository with {BackgroundColors.CYAN}{unprocessed_commits}{BackgroundColors.GREEN} unprocessed commits...{Style.RESET_ALL}")
            {key: True for key in RUN_FUNCTIONS} # Set all functions to True
      else:
         print(f"{BackgroundColors.RED}The {BackgroundColors.CYAN}{repository_name}{BackgroundColors.RED} metrics directory is incomplete or missing files.{Style.RESET_ALL}")

   process_repository(repository_name, repository_url, number_of_commits)

def process_repositories_in_parallel():
   """
   Processes each repository in the DEFAULT_REPOSITORIES dictionary in parallel using a thread pool.

   :return: None
   """

   print(f"{BackgroundColors.GREEN}Processing each of the repositories in parallel using a Thread Pool...{Style.RESET_ALL}")

   cpu_cores = get_threads() # Get the number of CPU cores
   usable_threads, max_threads = get_adjusted_number_of_threads(cpu_cores) # Get the adjusted number of threads to use

   print(f"{BackgroundColors.GREEN}The number of usable threads is {BackgroundColors.CYAN}{usable_threads}{BackgroundColors.GREEN} out of {BackgroundColors.CYAN}{max_threads}{BackgroundColors.GREEN}.{Style.RESET_ALL}")

   def process_and_estimate(repository_name, repository_url): # Function to process each repository and estimate the time
      estimated_time_string = f"{BackgroundColors.GREEN}Estimated time for running all iterations for {BackgroundColors.CYAN}{repository_name}{BackgroundColors.GREEN}: "

      # Traverse commits in the repository and count them without materializing the entire list
      number_of_commits = sum(1 for _ in Repository(repository_url).traverse_commits()) # Efficient commit count
      estimated_time = round(((number_of_commits / 1000) * number_of_commits), 2) # Estimate the time to process the repository
      output_time(estimated_time_string, estimated_time) # Output the estimated time
      setup_process_repository(repository_name, repository_url, number_of_commits) # Process the repository

   # Use ThreadPoolExecutor with a limit of usable_threads
   with concurrent.futures.ThreadPoolExecutor(max_workers=usable_threads) as executor:
      futures = [ # Create a list of futures. Futures are used to manage the execution of the function in the thread pool
         executor.submit(process_and_estimate, repository_name, repository_url) # Submit the process_and_estimate function
         for repository_name, repository_url in DEFAULT_REPOSITORIES.items() # Loop through the DEFAULT_REPOSITORIES dictionary
      ]
      concurrent.futures.wait(futures) # Wait for all tasks to complete

atexit.register(play_sound) # Register the function to play a sound when the program finishes

def main():
   """
   Main function.

   :return: None
   """

   start_time = datetime.now() # Get the start time
   
   if path_contains_whitespaces(): # Verify if the path constants contains whitespaces
      print(f"{BackgroundColors.RED}The {START_PATH} constant contains whitespaces. Please remove them!{Style.RESET_ALL}")
      return # Return if the path constants contains whitespaces

   global SOUND_FILE_PATH # Declare the SOUND_FILE_PATH as a global variable
   SOUND_FILE_PATH = update_sound_file_path() # Update the sound file path
   
   if not verify_git(): # Verify if Git is installed
      return # Return if Git is not installed
   
   if RUN_FUNCTIONS["CK Metrics"] and not ensure_ck_jar_file_exists(): # Verify and ensure that the CK JAR file exists
      return # Return if the CK JAR file does not exist

   verify_repositories_execution_constants() # Verify the DEFAULT_REPOSITORIES constant
   
   # Print the Welcome Messages
   print(f"{BackgroundColors.GREEN}Welcome to the {BackgroundColors.CYAN}CK Metrics Generator{BackgroundColors.GREEN}! This script is a key component of the {BackgroundColors.CYAN}Worked Example Miner (WEM) Project{BackgroundColors.GREEN}.{Style.RESET_ALL}")
   print(f"{BackgroundColors.GREEN}This script will process the repositories: {BackgroundColors.CYAN}{', '.join(repo.capitalize() for repo in DEFAULT_REPOSITORIES.keys())}{BackgroundColors.GREEN} in parallel using threads.{Style.RESET_ALL}")
   print(f"{BackgroundColors.GREEN}The files that this script will generate are the {BackgroundColors.CYAN}ck metrics files, the commit hashes list file and the diffs of each commit{BackgroundColors.GREEN}, in which are used by the {BackgroundColors.CYAN}Metrics Changes{BackgroundColors.GREEN} Python script.{Style.RESET_ALL}", end="\n\n")   

   process_repositories_in_parallel() # Process each of the repositories in parallel

   end_time = datetime.now() # Get the end time
   output_time(f"\n{BackgroundColors.GREEN}Total execution time: ", (end_time - start_time).total_seconds()) # Output the total execution time

   # Print the message that the CK metrics generator has finished processing the repositories
   print(f"\n{BackgroundColors.GREEN}The {BackgroundColors.CYAN}CK Metrics Generator{BackgroundColors.GREEN} has finished processing the {BackgroundColors.CYAN}{', '.join(repo.capitalize() for repo in DEFAULT_REPOSITORIES.keys())}{BackgroundColors.GREEN} repositories.{Style.RESET_ALL}", end="\n\n")
		
if __name__ == "__main__":
   """
   This is the standard boilerplate that calls the main() function.

   :return: None
   """

   main() # Call the main function
